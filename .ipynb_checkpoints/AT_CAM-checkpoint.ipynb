{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import walk\n",
    "import math\n",
    "\n",
    "import cv2 as cv\n",
    "import networkx as nx\n",
    "\n",
    "import chart_studio.plotly as py\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "#from Shapes import shapes\n",
    "#from Lines import lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/media/carterrhea/Dropbox/APLS-CAM-Proposal/DataFinal/Clean' #path to data on local computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get unique elements from a list\n",
    "def list_unique(list1): \n",
    "  \n",
    "    # intilize a null list \n",
    "    unique_list = [] \n",
    "      \n",
    "    # traverse for all elements \n",
    "    for x in list1: \n",
    "        # check if exists in unique_list or not \n",
    "        if x not in unique_list: \n",
    "            unique_list.append(x) \n",
    "            \n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_id_list(my_file_path):\n",
    "\n",
    "    #pull all filenames in your data directory\n",
    "    _, _, filenames = next(walk(my_file_path))\n",
    "\n",
    "    #grab only the id from the filenames to feed to the function\n",
    "    id_list = []\n",
    "    for filename in filenames:\n",
    "        filename_split = filename.rsplit('_', 1)\n",
    "        id_list.append(filename_split[0])\n",
    "    \n",
    "    unique_id = list_unique(id_list) #drop duplicate ids, since they are the same for _blocks & _links\n",
    "        \n",
    "    \n",
    "    \n",
    "    return unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get an id_list from your specific file path\n",
    "my_id_list = make_id_list(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at total number of cams\n",
    "len(my_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at a single CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>x_pos</th>\n",
       "      <th>y_pos</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>shape</th>\n",
       "      <th>creator</th>\n",
       "      <th>num</th>\n",
       "      <th>comment</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>modifiable</th>\n",
       "      <th>CAM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, title, x_pos, y_pos, width, height, shape, creator, num, comment, timestamp, modifiable, CAM]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>starting_block</th>\n",
       "      <th>ending_block</th>\n",
       "      <th>line_style</th>\n",
       "      <th>creator</th>\n",
       "      <th>num</th>\n",
       "      <th>arrow_type</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>CAM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, starting_block, ending_block, line_style, creator, num, arrow_type, timestamp, CAM]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cam_id = my_id_list[3] #grab a single id to just look through code\n",
    "\n",
    "blocks = pd.read_csv(f'{file_path}/{cam_id}_blocks.csv')\n",
    "links = pd.read_csv(f'{file_path}/{cam_id}_links.csv')\n",
    "\n",
    "display(blocks.head())\n",
    "display(links.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create A Graph Using Networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to build most basic graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_from_df(node_df, edge_df):\n",
    "    # Get nodes\n",
    "    nodes = node_df['id'].to_list()\n",
    "    \n",
    "    # Get edges\n",
    "    edge_start = edge_df['starting_block'].to_list()\n",
    "    edge_end = edge_df['ending_block'].to_list()\n",
    "    edges = tuple(zip(edge_start, edge_end))\n",
    "    \n",
    "    # Create Graph\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from(edges)\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = graph_from_df(blocks,links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to add attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_node_attributes(graph_name, node_df, attr_name): #attr_column_name must be in quotes\n",
    "    \n",
    "    blocks_dict = node_df[['id', attr_name]].set_index('id').to_dict()[attr_name]\n",
    "    \n",
    "    nx.set_node_attributes(graph_name, blocks_dict, attr_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_link_attributes(graph_name, edge_df, attr_name):\n",
    "    attr_full_dict = {}\n",
    "    \n",
    "    edge_df['unique_link']= list(zip(edge_df['starting_block'], edge_df['ending_block']))\n",
    "        \n",
    "    for idx, row in edge_df.iterrows():\n",
    "        \n",
    "        attr_dict = {attr_name: row[attr_name]}\n",
    "        \n",
    "        attr_full_dict.update({row['unique_link']:attr_dict})\n",
    "    \n",
    "    nx.set_edge_attributes(G, attr_full_dict)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make all the Graphs\n",
    "#### Store all graphs with attributes in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAM 575ec4be3fc3ac000611df4c__771 does not exist\n",
      "Summary Info:\n",
      "number of missing cams: 1\n",
      "number of cams with no links: 32\n",
      "number of complete cams: 126\n"
     ]
    }
   ],
   "source": [
    "graph_list = []\n",
    "working_id = []\n",
    "no_data = []\n",
    "missing_cam = []\n",
    "\n",
    "for i in my_id_list:\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        blocks_df = pd.read_csv(f'{file_path}/{i}_blocks.csv')\n",
    "\n",
    "        links_df = pd.read_csv(f'{file_path}/{i}_links.csv')\n",
    "        \n",
    "        if len(links_df)>0:\n",
    "\n",
    "            G = graph_from_df(blocks_df, links_df)\n",
    "\n",
    "            add_node_attributes(G,blocks_df,'title')\n",
    "\n",
    "            add_node_attributes(G,blocks_df,'shape')\n",
    "\n",
    "            add_link_attributes(G, links_df, 'line_style')\n",
    "\n",
    "            graph_list.append(G)\n",
    "            \n",
    "            working_id.append(i)\n",
    "\n",
    "        \n",
    "        else:\n",
    "            no_data.append(i)\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        missing_cam.append(i)\n",
    "        print(f'CAM {i} does not exist')\n",
    "\n",
    "print('Summary Info:')\n",
    "print(f'number of missing cams: {len(missing_cam)}')\n",
    "print(f'number of cams with no links: {len(no_data)}')\n",
    "print(f'number of complete cams: {len(working_id)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at one of the graphs stored in your graph list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14072, 14085, {'line_style': 'Solid-Weak'}), (14072, 14257, {'line_style': 'Solid-Weak'}), (14072, 14272, {'line_style': 'Solid-Weak'}), (14257, 14272, {'line_style': 'Solid'})]\n",
      "-------------------------\n",
      "[(14072, {'title': 'shopping at the farmers´ market', 'shape': 'neutral'}), (14085, {'title': 'outdoors', 'shape': 'ambivalent'}), (14257, {'title': 'more expensive', 'shape': 'negative strong'}), (14272, {'title': 'regional', 'shape': 'positive strong'})]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_graph = graph_list[1]\n",
    "\n",
    "nx.draw(my_graph)\n",
    "print(my_graph.edges(data = True))\n",
    "print('-------------------------')\n",
    "print(my_graph.nodes(data = True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diversity Measure\n",
    "Carter 02/19/21 \n",
    "\n",
    "This comes from pg 5 of the following paper: https://arxiv.org/pdf/0901.1380.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distance matrix\n",
    "D = np.zeros((7,7))  # We have seven node types hence 7 (note that we consider ambivalent as the same as neutral in this calculation)\n",
    "dist_ = np.arange(7)\n",
    "for i in range(7):\n",
    "    D[i,i:] = dist_\n",
    "    D[i:,i] = dist_\n",
    "    dist_ = dist_[:-1]\n",
    "\n",
    "def calculate_diversity(D, probs):\n",
    "    \"\"\"\n",
    "    Calculate Stirling Diversity measure with alpha = beta = 1\n",
    "    Args:\n",
    "        param: D - Distance Matrix\n",
    "        param: probs - list of probabilities in order of Strong Negative, Negative, Weak Negative, Neutral, Weak Positive, Positive, Strong Positive\n",
    "    \"\"\"\n",
    "    S_ = 0\n",
    "    for i in range(7):\n",
    "        for j in range(7):\n",
    "            S += D[i, j]*probs[i]*probs[j]\n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assumption: graph probability distribution is uniform, conditioned on size and density\n",
    " (all graphs of given size and density are equally probable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G n,M = graph of size n, with M links \n",
    "Sample procedure:\n",
    "create an empty graph with n vertices and randomly insert M links from n(n-1) possible links without replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to make data frame of GLI for all CAMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_CAM_GLI(id_list):\n",
    "    positive_list = ['positive strong', 'positive','positive weak']\n",
    "    negative_list = ['negative strong','negative','negative weak']\n",
    "    neutral_list = ['neutral','ambivalent']\n",
    "    \n",
    "    solid_list = ['Solid', 'Solid-Strong', 'Solid-Weak']  # Solid link types\n",
    "    dashed_list = ['Dashed', 'Dashed-Strong', 'Dashed-Weak']  # Dashed link types\n",
    "    \n",
    "    dictionary_list = []\n",
    "    \n",
    "    for cam_id in id_list:\n",
    "        block_neg = 0\n",
    "        block_pos = 0\n",
    "        block_neut = 0\n",
    "        \n",
    "        link_solid = 0\n",
    "        link_dashed = 0\n",
    "        \n",
    "        try:\n",
    "            df_blocks = pd.read_csv(f'{file_path}/{cam_id}_blocks.csv')\n",
    "            df_links = pd.read_csv(f'{file_path}/{cam_id}_links.csv')\n",
    "            \n",
    "            #Get proportion of positive and negative nodes\n",
    "            for i in range(len(df_blocks)):\n",
    "                if df_blocks['shape'][i] in negative_list:\n",
    "                    block_neg = block_neg + 1\n",
    "                elif df_blocks['shape'][i] in positive_list:\n",
    "                    block_pos = block_pos + 1\n",
    "                else:\n",
    "                    block_neut = block_neut + 1\n",
    "            \n",
    "            # Get proportion of solid and dashed lines\n",
    "            for i in range(len(df_links)):\n",
    "                if df_link['line_style'][i] in solid_list:\n",
    "                    link_solid += 1\n",
    "                else:\n",
    "                    link_dashed += 1\n",
    "        \n",
    "            # Get probabilities of each type of node -- used for diversity calculation\n",
    "            neg_strong = df[df['shape']=='negative strong'].count()\n",
    "            neg = df[df['shape']=='negative'].count()\n",
    "            neg_weak = df[df['shape']=='negative weak'].count()\n",
    "            neutral = df[(df['shape']=='neutral') & (df['shape']=='ambivalent')].count()\n",
    "            pos_weak = df[df['shape']=='positive weak'].count()\n",
    "            pos = df[df['shape']=='positive'].count()\n",
    "            pos_strong = df[df['shape']=='positive strong'].count()\n",
    "            probs = [neg_strong, neg, neg_weak, neutral, pos_weak, pos, pos_strong]\n",
    "            div_ = calculate_diversity(D, probs)\n",
    "            \n",
    "            # Get nodes\n",
    "            nodes = df_blocks['id'].to_list()\n",
    "            \n",
    "            # Get edges\n",
    "            edge_start = df_links['starting_block'].to_list()\n",
    "            edge_end = df_links['ending_block'].to_list()\n",
    "            edges = tuple(zip(edge_start, edge_end))\n",
    "            \n",
    "            # Create Graph\n",
    "            G = nx.Graph()\n",
    "            G.add_nodes_from(nodes)\n",
    "            G.add_edges_from(edges)\n",
    "            \n",
    "            # Calculate Density\n",
    "            density = np.round(nx.density(G), 3)\n",
    "            node_count = G.number_of_nodes()\n",
    "            edge_count = G.size()\n",
    "            \n",
    "            # Calculate longest path\n",
    "            try:\n",
    "                components = nx.connected_components(G)\n",
    "                largest_component = max(components, key=len)\n",
    "                subgraph = G.subgraph(largest_component)\n",
    "                diameter = nx.diameter(subgraph)\n",
    "            except:\n",
    "                diameter = 0\n",
    "            # Calculate transitivity\n",
    "            triadic_closure = np.round(nx.transitivity(G), 3)\n",
    "            \n",
    "            # Calculate max degree\n",
    "            try:\n",
    "                degree_centrality = nx.degree_centrality(G)\n",
    "                max_centrality_ind = np.argmax(list(degree_centrality.values()))\n",
    "                central_node = list(degree_centrality.keys())[max_centrality_ind]\n",
    "                central_node_title = df_blocks[df_blocks['id'] == central_node]['title'].values[0]\n",
    "                central_node_val = np.round(list(degree_centrality.values())[max_centrality_ind], 3)\n",
    "            except:\n",
    "                central_node = 0\n",
    "                central_node_title = ''\n",
    "                central_node_val = 0\n",
    "                \n",
    "            # Eigenvector Centrality\n",
    "            try:\n",
    "                eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "                max_centrality_ind = np.argmax(list(eigenvector_centrality.values()))\n",
    "                central_node_val_eig = np.round(list(eigenvector_centrality.values())[max_centrality_ind], 3)\n",
    "            except:\n",
    "                central_node_val_eig = 0\n",
    "                \n",
    "            # Betweeness Centrality\n",
    "            try:\n",
    "                betweenness_centrality = nx.betweenness_centrality(G)\n",
    "                max_centrality_ind = np.argmax(list(betweenness_centrality.values()))\n",
    "                central_node_val_bet = np.round(list(betweenness_centrality.values())[max_centrality_ind], 3)\n",
    "            except:\n",
    "                central_node_val_bet = 0\n",
    "\n",
    "            # Make the dictionary\n",
    "                #add in ratio of node val\n",
    "                #add number of type of node\n",
    "            density_dict = {\n",
    "                'cam_id': cam_id,\n",
    "                'node_count': node_count,\n",
    "                'positive_nodes':block_pos,\n",
    "                'negative_nodes':block_neg,\n",
    "                'neutral_nodes':block_neut,\n",
    "                'link_solid': link_solid,\n",
    "                'link_dashed': link_dashed,\n",
    "                'edge_count': edge_count,\n",
    "                'density': density,\n",
    "                'diameter': diameter,\n",
    "                'triadic_closure': triadic_closure,\n",
    "                'central_node': central_node,\n",
    "                'central_node_title': central_node_title,\n",
    "                'central_node_val': central_node_val,\n",
    "                'central_node_val_eig': central_node_val_eig,\n",
    "                'central_node_val_bet': central_node_val_bet,\n",
    "                'diversity': div_\n",
    "            }\n",
    "\n",
    "            dictionary_list.append(density_dict)\n",
    "            \n",
    "        except:\n",
    "            print(f'No file found for {cam_id}')\n",
    "        \n",
    "    cam_density_df = pd.DataFrame(dictionary_list)\n",
    "            \n",
    "    return cam_density_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file found for 5f6c238ed9915e19408f161b_948\n",
      "No file found for 5e30a49f16d4ec0a91a513c8_934\n",
      "No file found for 5f6f8cc1d1db335366cbf037_964\n",
      "No file found for 5e80cd7d5a8b770009b6884e_879\n",
      "No file found for 5c80a72a979a7b0001f3c57b_882\n",
      "No file found for 5f6f8cc1d1db335366cbf037_949\n",
      "No file found for 5e30a49f16d4ec0a91a513c8_971\n",
      "No file found for 5f6f8cc1d1db335366cbf037_951\n",
      "No file found for 5f2b3a8a7823df56b6f5ce3b_965\n",
      "No file found for 5f6f8cc1d1db335366cbf037_930\n",
      "No file found for 5acaf4b1e1546900019c388c_927\n",
      "No file found for 5df5b73cd52c7b4021e0d0b6_985\n",
      "No file found for 5e30a49f16d4ec0a91a513c8_932\n",
      "No file found for 5e30a49f16d4ec0a91a513c8_933\n",
      "No file found for cacc_1013\n",
      "No file found for 5f6f8cc1d1db335366cbf037_938\n",
      "No file found for 5eeae82d741eed221ec5a245_911\n",
      "No file found for 5f8dd7faaf903e14492bd807_983\n",
      "No file found for 5c67e30581ea8900018ae0d0_863\n",
      "No file found for 5f7d2119fc535d1fc42e1b71_775\n",
      "No file found for 5edd6f885833dea763d43dfa_742\n",
      "No file found for 5f6f8cc1d1db335366cbf037_936\n",
      "No file found for 5e30a49f16d4ec0a91a513c8_970\n",
      "No file found for 5e80cd7d5a8b770009b6884e_880\n",
      "No file found for 5f6f8cc1d1db335366cbf037_937\n",
      "No file found for 5f6f8cc1d1db335366cbf037_972\n",
      "No file found for 5da64c3721d74a0016dfce74_902\n",
      "No file found for 5f279e5e7f5aab5ce9a65248_869\n",
      "No file found for 5ebc3097a9c38504ebd8a347_862\n",
      "No file found for 5f6c238ed9915e19408f161b_942\n",
      "No file found for 5ef8a0b375e30603ebc12510_772\n",
      "No file found for 5ef10039d2f5e76c90c4ddb3_889\n",
      "No file found for 5f6c238ed9915e19408f161b_931\n",
      "No file found for 5fc92bd24a099c36f190c0ee_900\n",
      "No file found for 59ee2a30ae9f950001d849bf@email.prolific.co_888\n",
      "No file found for 5e94ae0942eb133064297a25_891\n",
      "No file found for 5ed038155becab105de8d74a_759\n",
      "No file found for 5f7ca372a8afc510abbbe99b_886\n",
      "No file found for 5ee9e12423e2ad000ba4fe15_960\n",
      "No file found for 5f9851db9003f30f0baeb38a_867\n",
      "No file found for 5a5e51de76d1c60001ab65b9_840\n",
      "No file found for 5ea1a9b5e876fe07b737367b_865\n",
      "No file found for 5ebb97415b160116ba6d1df3_984\n",
      "No file found for 5f27916a60644b5aabb1e883_743\n",
      "No file found for 5e8e5216775eaa111a50f692_906\n",
      "No file found for 5f71f79cb0d9ac09c597530b_860\n",
      "No file found for 5f1b14b5f904e60fba8b4e70_821\n",
      "No file found for 5656389d995911000f482b7d_878\n",
      "No file found for 5bc0cacb228a7600013cf77e_857\n",
      "No file found for 5f00a4aa972c080591ade7c3_807\n",
      "No file found for 5ec5da253792714cfc4063f3_977\n",
      "No file found for 5e49e321f03209000b790927_1005\n",
      "No file found for 5bf461fc707ccc00011cfb75_791\n",
      "No file found for 5ec0f11edf781a0009b0803d_975\n",
      "No file found for 57850499eeaf6d00018d3da3_805\n",
      "No file found for 5e59231c14a6a41332efb648_1008\n",
      "No file found for 5ca6c9f3a752220001ca1be3_838\n",
      "No file found for 5c9d28816b36c400013cc430_752\n",
      "No file found for 5f8848aec378732c550c555a_998\n",
      "No file found for 5f60219e6b25af11cc6d3772_803\n",
      "No file found for 5e86c11942701c2ffda5d113_997\n",
      "No file found for 5f27049070a6d24b85293515_895\n",
      "No file found for 5ebf37122a92681d00c3c9e8_747\n",
      "No file found for 5f769d9142962d14bdb60b1f_922\n",
      "No file found for 5e599c42a6f0b31b0e9f68d5_954\n",
      "No file found for 5ecf1fb0eed497344398139d_988\n",
      "No file found for TysonDueck_1004\n",
      "No file found for 5d810cd0a9311300170dbf8a_982\n",
      "No file found for 5bc8cdca0f10750001d70716_851\n",
      "No file found for 5fc12002ae4b06000b8211ea_950\n",
      "No file found for 5ec5faacab872c67dfb0f505_868\n",
      "No file found for 5ebc3097a9c38504ebd8a347_861\n",
      "No file found for 5e41c1814a5479054e0fa937_833\n",
      "No file found for 5de1d1a310088f22f7ce0588_952\n",
      "No file found for 5f7dd740074da20008c51bf5_818\n",
      "No file found for 5e799942bc70c9559d6b845a_823\n",
      "No file found for 5f2dc68fb68d470fe02d63b8_836\n",
      "No file found for 5db54b75a3df85000edfa2cf_788\n",
      "No file found for 5ab1470124ba100001a0456a_835\n",
      "No file found for 5e7eed034ba18a5b7a390db7_901\n",
      "No file found for 5f32a496a026e809d7d55cbd_817\n",
      "No file found for 5f764c576fc02d0cfdae9a94_829\n",
      "No file found for 5f4bd2bc2619997c752dffe0_914\n",
      "No file found for 5e45dd10f354ca0c7f4225c7_903\n",
      "No file found for 5b0c84ed30d5620001555579_765\n",
      "No file found for 5f4f4f3e3e7b0801bc3faccc_955\n",
      "No file found for 5e8e63f0e502b1119e4cdc32_896\n",
      "No file found for 5f529beee27bc758118caa9c_961\n",
      "No file found for 5f9261c7e1a8d117eadb3ba9_793\n",
      "No file found for 5c732f631388ff0001100e6d_783\n",
      "No file found for 5d34f1a7f7b9c100155ae5b5_875\n",
      "No file found for 5a4e8f4930adf7000106fd5e_928\n",
      "No file found for Emmett_887\n",
      "No file found for 5ef8a0b375e30603ebc12510_774\n",
      "No file found for 5fc34b792e8c1e614dc36eb0_989\n",
      "No file found for 5ded3d21be28563ff17ee400_1006\n",
      "No file found for 5f67f8d55e7718150fd23bc7_802\n",
      "No file found for 5d5ab9dac4866400012bcde2_993\n",
      "No file found for 5f6bb67a746957114102e0ff_800\n",
      "No file found for 5f90d327ff46261f08ec2b2a_844\n",
      "No file found for 5f3c4372d64ff120268a0b0b_994\n",
      "No file found for 5fc728cc92a39e10b2b4b727_980\n",
      "No file found for 5e9493b01918212de4d879aa_825\n",
      "No file found for 5de5bf071a7af854f0910893_824\n",
      "No file found for 5f4a274f21ec8a5971fbb33f_894\n",
      "No file found for 5c9a8b96fbcf4a001747d59b_782\n",
      "No file found for 5d01553397491100194a508b_892\n",
      "No file found for 575ec4be3fc3ac000611df4c__771\n",
      "No file found for 5d1d49b9bfcb7c001acc9a34_864\n",
      "No file found for 5f53bb278de59a6d87e53aab_784\n",
      "No file found for 5eed1ee2cb93ea12dfc13405_899\n",
      "No file found for 5c2d3ba4731cd10001b647d7_884\n",
      "No file found for 5f46f2f57385380d78983337_919\n",
      "No file found for 5a8068eb000dab00018c6fb6_915\n",
      "No file found for 5ecc47c44bd36744c20ba660_778\n",
      "No file found for 5f3b08c306616c805cd82c15_908\n",
      "No file found for 5aeb4e9b08e2f00001083199_848\n",
      "No file found for 5ef756bd60ee3e03dea31025_935\n",
      "No file found for 5f04cb284a1b587a33e1978d_1000\n",
      "No file found for 5e939dee9da60e6d9a56a710_926\n",
      "No file found for 5f04cb284a1b587a33e1978d_1002\n",
      "No file found for 5f04cb284a1b587a33e1978d_1001\n",
      "No file found for 5f4ae6a7ad7baa6aecef4234_990\n",
      "No file found for 5fe3c20fefea400a5000d2ff_974\n",
      "No file found for 5fc5479e0c13f804b402c760_811\n",
      "No file found for 5e91892766838a4bf681e692_809\n",
      "No file found for 5f1725b16e58fa0316af9bfb_973\n",
      "No file found for 5f5b56044cddcc2735573f65_991\n",
      "No file found for 5f63ef0a7f3a881644caffe1_849\n",
      "No file found for 5b5e6db65256470001bc902e_890\n",
      "No file found for 5e6473fbc15dbe1f71eea95b_909\n",
      "No file found for 5fbd817adc800416b8bddf2f_905\n",
      "No file found for 5f2b3a8a7823df56b6f5ce3b_966\n",
      "No file found for 5f7e700665a5f514520a6f80_885\n",
      "No file found for 5f7b6f66f815c412bc31bc7c_999\n",
      "No file found for 5dee83801fd6b3000bc9d01a_787\n",
      "No file found for 5c0ef103857373000152f759_834\n",
      "No file found for 5cae3985d14c8a0017678f48_913\n",
      "No file found for 5f57fef7e7b06a35296f215d_822\n",
      "No file found for 5fef51d5b890b1d99daa1fd0_745\n",
      "No file found for 5f4d2991a6732099daa6e1cb_1007\n",
      "No file found for 5faa7a04cb538f398cde6679_808\n",
      "No file found for 5f7bdf989dc5621cb03890cf_969\n",
      "No file found for 5f32f4f2edba920cca1a2051_898\n",
      "No file found for 5f86545822cf5c1a8f21ad3d_929\n",
      "No file found for 5ea302aeb39fb424ac3b7e36_837\n",
      "No file found for 5e9f84b2ccad1912515e80f4_866\n",
      "No file found for 5b0f33dd1e55760001b9a9ce_812\n",
      "No file found for 5ecc0ae50cf6970c228a6bd6_820\n",
      "No file found for 5ff9da6b7d5a4e31c52f75e8_750\n",
      "No file found for 5cf531a9d8b79300013b83c9_810\n",
      "No file found for 5e14147d839079a4b7d535f5_968\n",
      "No file found for 5c47784e85a4920001eb885a_754\n",
      "No file found for 5f8dd7faaf903e14492bd807_981\n",
      "No file found for 5d316c4442c2170018e10574_839\n",
      "No file found for 5f04d24b25f8c47a4215f2ff_790\n",
      "No file found for 590819e205fbfc000147720c_925\n",
      "No file found for 5f626742d710062b8443238c_893\n",
      "No file found for 5ff183fa4b3bdc583d7bcf73_978\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cam_GLI_df = calc_CAM_GLI(my_id_list)\n",
    "display(cam_GLI_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'node_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-06850f99ca8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#get a df of the number of cams per node count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnode_count_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam_GLI_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'node_count'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mas_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'node_count'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'edge_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'edge_count'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'cam_count'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnode_count_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[1;32m   6513\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6515\u001b[0;31m         return DataFrameGroupBy(\n\u001b[0m\u001b[1;32m   6516\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6517\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_grouper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[1;32m    526\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[1;32m    784\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'node_count'"
     ]
    }
   ],
   "source": [
    "#get a df of the number of cams per node count\n",
    "node_count_df = pd.DataFrame(cam_GLI_df.groupby('node_count',as_index=False).count()[['node_count','edge_count']]).rename(columns={'edge_count':'cam_count'})\n",
    "node_count_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at distribution of node count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(5,5),sharey=True, tight_layout=True)\n",
    "\n",
    "# We can set the number of bins with the `bins` kwarg\n",
    "axs.hist(cam_GLI_df['node_count'])\n",
    "axs.set_title('Distribution of Node Count')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare GLI from CAM with what would be expected from chance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anderson et al. Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = list(cam_GLI_df.columns)\n",
    "GLI_list = col_list[8:]\n",
    "GLI_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cam_GLI_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLI_name = GLI_list[1]\n",
    "\n",
    "P_H = 0\n",
    "P_L = 0\n",
    "P_E = 0\n",
    "N = 0 \n",
    "\n",
    "N_max = 100 #number of iterations\n",
    "\n",
    "alpha = .05 # desired level of significance\n",
    "\n",
    "condition_list = []\n",
    "sig_cam_list = []\n",
    "nonsig_cam_list = []\n",
    "\n",
    "for i in range(len(cam_GLI_df)):\n",
    "    \n",
    "    Obs_GLI = cam_GLI_df[GLI_name][i] #observed GLI from row i\n",
    "\n",
    "    n = cam_GLI_df['node_count'][i] # number of nodes for cam i\n",
    "    m = cam_GLI_df['edge_count'][i]  # number of edges for cam i\n",
    "\n",
    "    while N < N_max:\n",
    "\n",
    "        # STEP 2: Make the random graph\n",
    "        G_uni = nx.gnm_random_graph(n, m)\n",
    "\n",
    "\n",
    "        #Get the necessary GLI measures\n",
    "        # Calculate Density\n",
    "        density = np.round(nx.density(G_uni), 3)\n",
    "\n",
    "        # Calculate longest path\n",
    "        try:\n",
    "            components = nx.connected_components(G_uni)\n",
    "            largest_component = max(components, key=len)\n",
    "            subgraph = G_uni.subgraph(largest_component)\n",
    "            diameter = nx.diameter(subgraph)\n",
    "        except:\n",
    "            diameter = 0\n",
    "        # Calculate transitivity\n",
    "        triadic_closure = np.round(nx.transitivity(G_uni), 3)\n",
    "\n",
    "        # Calculate max degree\n",
    "        try:\n",
    "            degree_centrality = nx.degree_centrality(G_uni)\n",
    "            max_centrality_ind = np.argmax(list(degree_centrality.values()))\n",
    "            central_node_val = np.round(list(degree_centrality.values())[max_centrality_ind], 3)\n",
    "        except:\n",
    "            central_node_val = 0\n",
    "\n",
    "        # Eigenvector Centrality\n",
    "        try:\n",
    "            eigenvector_centrality = nx.eigenvector_centrality(G_uni)\n",
    "            max_centrality_ind = np.argmax(list(eigenvector_centrality.values()))\n",
    "            central_node_val_eig = np.round(list(eigenvector_centrality.values())[max_centrality_ind], 3)\n",
    "        except:\n",
    "            central_node_val_eig = 0\n",
    "\n",
    "        # Betweeness Centrality\n",
    "        try:\n",
    "            betweenness_centrality = nx.betweenness_centrality(G_uni)\n",
    "            max_centrality_ind = np.argmax(list(betweenness_centrality.values()))\n",
    "            central_node_val_bet = np.round(list(betweenness_centrality.values())[max_centrality_ind], 3)\n",
    "        except:\n",
    "            central_node_val_bet = 0\n",
    "\n",
    "        # Make the dictionary\n",
    "        G_uni_GLI_dict = {\n",
    "            'central_node_val': central_node_val,\n",
    "            'central_node_val_eig': central_node_val_eig,\n",
    "            'central_node_val_bet': central_node_val_bet\n",
    "        }\n",
    "\n",
    "        #STEP 3 - #Maybe issue with have a two conditions with the 'equal to' option\n",
    "        \n",
    "        if G_uni_GLI_dict[GLI_name] >= Obs_GLI:\n",
    "            P_H = P_H + 1\n",
    "\n",
    "        elif G_uni_GLI_dict[GLI_name] <= Obs_GLI:\n",
    "            P_L = P_L +1\n",
    "\n",
    "        #increment N for each iteration till max is reached\n",
    "        N = N + 1\n",
    "\n",
    "    #Step 5 - This tests whether the observed GLI is consistently either higher or lower than the GLI predicted\n",
    "    # by a random graph. \n",
    "    condition_1 = (P_H/N_max) < (alpha/2)\n",
    "    condition_2 = (P_L/N_max) < (alpha/2)\n",
    "    \n",
    "    condition_list.append((P_H/N_max,P_L/N_max))\n",
    "\n",
    "    if condition_1 | condition_2:\n",
    "        \n",
    "        sig_cam_list.append(cam_GLI_df['cam_id'][i])\n",
    "    \n",
    "    else:\n",
    "        nonsig_cam_list.append(cam_GLI_df['cam_id'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " len(nonsig_cam_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Bays approach to disprove that size impacts \n",
    "build random graph with different node size\n",
    "create likelihood function\n",
    "\n",
    "G_uni = nx.gnm_random_graph(n, m) density fixed \n",
    "G = nx.gnp_random_graph(n,p) density varies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Survey Info to CAM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_survey = '/Users/agetecza/CAM_Data/AT_CAM/Data/CAM_Survey_Merged.csv'\n",
    "survey_df = pd.read_csv(file_path_survey)\n",
    "survey_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop NAs\n",
    "survey_df_dropna = survey_df.dropna(subset=['Prolific1','Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the cam dataframe ready to merge with the survey data\n",
    "cam_GLI_full = cam_GLI_df.copy()\n",
    "Prolific1_list=cam_GLI_full['cam_id'].str.split('_').str[0]\n",
    "cam_GLI_full.insert(loc=0, column='Prolific1', value=Prolific1_list)\n",
    "cam_GLI_full = cam_GLI_full.drop(cam_GLI_full.columns[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The length of cam_df: {len(cam_GLI_full)}')\n",
    "print(f'The length of survey_df: {len(survey_df_dropna)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_cam_merge = pd.merge(cam_GLI_full, survey_df_dropna, how = 'inner', on = 'Prolific1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The length of merged data: {len(survey_cam_merge)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the combined dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop all rows with node_count less than 3 or link_count less than 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_cam = (survey_cam_merge['node_count']>3) & (survey_cam_merge['edge_count']>2)\n",
    "survey_merge_large_cam = survey_cam_merge[filter_cam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(survey_merge_large_cam)} rows have more than 3 nodes and more than 2 edges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Survey checks???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Variables for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of all column/variables names\n",
    "survey_column_list = list(survey_merge_large_cam.columns)\n",
    "#survey_column_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the index of a specific variable\n",
    "survey_column_list.index('KNRef') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the controls\n",
    "control_list_1 = ['Age','Education','Income','English','Gender','CarbFam','CarbSFam']\n",
    "control_list_Cog = survey_column_list[108:126]\n",
    "control_list_Aff = survey_column_list[126:152]\n",
    "control_list_KN = survey_column_list[173:180]\n",
    "\n",
    "control_list = control_list_1 + control_list_Aff + control_list_Cog + control_list_KN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all variable\n",
    "network_list = survey_column_list[:14]\n",
    "dependent_list = ['CarbAg','Interest_1','CarbDecide']\n",
    "full_variable_list = network_list + dependent_list + control_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset df \n",
    "survey_df_analysis = survey_merge_large_cam.loc[:,full_variable_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_list = list(survey_df_analysis.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame(survey_df_analysis[variable_list].describe()).transpose()\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation & Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up column order\n",
    "#column_order = list(np.arange(0,21))+list(np.arange(47,50))+list(np.arange(65,68))+list(np.arange(24,47))+list(np.arange(50,65))+list(np.arange(21,24))+list(np.arange(68,72))\n",
    "#survey_df_arrange = survey_df_analysis.iloc[:,column_order]\n",
    "#survey_df_arrange.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = survey_df_analysis.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix.iloc[7,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = pd.DataFrame(columns = ['row','column','corr'])\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(len(correlation_matrix)):\n",
    "        if (correlation_matrix.iloc[i,j]>.6) & (correlation_matrix.iloc[i,j]!=1):\n",
    "            newrow = {'row':correlation_matrix.columns[i],'column':correlation_matrix.columns[j],'corr':correlation_matrix.iloc[i,j]}\n",
    "            corr_df = corr_df.append(newrow, ignore_index=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.subplots(figsize=(25,15))\n",
    "sns.heatmap(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(survey_df_analysis.columns).index('Aff1R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolate variables for individual heat mapping\n",
    "Cog_index = list(np.arange(list(survey_df_analysis.columns).index('Cog1'),list(survey_df_analysis.columns).index('Cog18')+1))\n",
    "Aff_index = list(np.arange(list(survey_df_analysis.columns).index('Aff1R'),list(survey_df_analysis.columns).index('Aff26')+1))\n",
    "Cog_df = survey_df_analysis.iloc[:,Cog_index]\n",
    "Aff_df = survey_df_analysis.iloc[:,Aff_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "sns.heatmap(Cog_df.corr(), ax=ax1)\n",
    "sns.heatmap(Aff_df.corr(), ax=ax2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "x_pca = pca.fit_transform(Aff_df)\n",
    "x_pca = pd.DataFrame(x_pca)\n",
    "x_pca.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(range(len(x_pca.columns)), explained_variance, alpha=0.5, align='center', label='individual variance')\n",
    "plt.legend()\n",
    "plt.ylabel('Variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check linear regression plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_index = variable_list.index('KNPrice') #Just enter the variable name you want an output for \n",
    "\n",
    "variable_name = variable_list[variable_index]\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(5,5),sharey=True, tight_layout=True)\n",
    "\n",
    "# We can set the number of bins with the `bins` kwarg\n",
    "axs.hist(survey_cam_merge[variable_name])\n",
    "axs.set_title(f'{variable_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create output of assumption test\n",
    "peirce test of normality for indep variable\n",
    "ind:\n",
    "dv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Are these switched?\n",
    "survey_dependent = survey_df_analysis[dependent_list]\n",
    "survey_network = survey_df_analysis[network_list]\n",
    "survey_c1 = survey_df_analysis[control_list_1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(survey_network.columns))\n",
    "survey_cent = survey_network.iloc[:,[6,13]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For all the below graphs:\n",
    "y = survey_dependent\n",
    "X = survey_cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "fig, axes = plt.subplots(nrows=len(y.columns), ncols=len(X.columns), figsize=(15, 10))\n",
    "                                   \n",
    "for i in range(len(y.columns)):\n",
    "    for j in range(len(X.columns)):\n",
    "        axes[i, j].scatter(X.iloc[:,j],y.iloc[:,i])\n",
    "        axes[i,j].set_xlabel(f'{X.columns[j]}',fontsize=10)\n",
    "        \n",
    "        \n",
    "        slope, inter, r, p, std_err = st.linregress(X.iloc[:,j],y.iloc[:,i])\n",
    "        plot_fit = slope * (X.iloc[:,j]) + inter\n",
    "        axes[i,j].plot((X.iloc[:,j]),plot_fit,linestyle='solid',color=\"red\",lw=.3)\n",
    "        \n",
    "        if j ==0:\n",
    "            axes[i,j].set_ylabel(f'{y.columns[i]}',fontsize=10)\n",
    "            \n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data transformations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boxcox shorthand:\n",
    "lambda = -1. is a reciprocal transform.\n",
    "lambda = -0.5 is a reciprocal square root transform.\n",
    "lambda = 0.0 is a log transform.\n",
    "lambda = 0.5 is a square root transform.\n",
    "lambda = 1.0 is no transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "data = boxcox(y.iloc[:,1], 0)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assumption Testing for OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "plt.style.use('seaborn') # pretty matplotlib plots\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('figure', titlesize=18)\n",
    "plt.rc('axes', labelsize=15)\n",
    "plt.rc('axes', titlesize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = survey_dependent.iloc[:,0].unique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = survey_cent\n",
    "X = sm.add_constant(X)\n",
    "y = survey_dependent.iloc[:,0]\n",
    "\n",
    "# generate OLS model\n",
    "model = sm.OLS(y, X)\n",
    "model_fit = model.fit()\n",
    "# create dataframe from X, y for easier plot handling\n",
    "dataframe = pd.concat([X, y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model values\n",
    "model_fitted_y = model_fit.fittedvalues\n",
    "# model residuals\n",
    "model_residuals = model_fit.resid\n",
    "# normalized residuals\n",
    "model_norm_residuals = model_fit.get_influence().resid_studentized_internal\n",
    "# absolute squared normalized residuals\n",
    "model_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n",
    "# absolute residuals\n",
    "model_abs_resid = np.abs(model_residuals)\n",
    "# leverage, from statsmodels internals\n",
    "model_leverage = model_fit.get_influence().hat_matrix_diag\n",
    "# cook's distance, from statsmodels internals\n",
    "model_cooks = model_fit.get_influence().cooks_distance[0]\n",
    "\n",
    "plot_lm_1 = plt.figure()\n",
    "plot_lm_1.axes[0] = sns.residplot(model_fitted_y, dataframe.columns[-1], data=dataframe,\n",
    "                          lowess=True,\n",
    "                          scatter_kws={'alpha': 0.5},\n",
    "                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n",
    "\n",
    "plot_lm_1.axes[0].set_title('Residuals vs Fitted')\n",
    "plot_lm_1.axes[0].set_xlabel('Fitted values')\n",
    "plot_lm_1.axes[0].set_ylabel('Residuals');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#further from line = errors not being normally distributed\n",
    "QQ = ProbPlot(model_norm_residuals)\n",
    "plot_lm_2 = QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1)\n",
    "plot_lm_2.axes[0].set_title('Normal Q-Q')\n",
    "plot_lm_2.axes[0].set_xlabel('Theoretical Quantiles')\n",
    "plot_lm_2.axes[0].set_ylabel('Standardized Residuals');\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The more horizontal the red line is, the more likely the data is homoscedastic\n",
    "plot_lm_3 = plt.figure()\n",
    "plt.scatter(model_fitted_y, model_norm_residuals_abs_sqrt, alpha=0.5);\n",
    "sns.regplot(model_fitted_y, model_norm_residuals_abs_sqrt,\n",
    "          scatter=False,\n",
    "          ci=False,\n",
    "          lowess=True,\n",
    "          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8});\n",
    "plot_lm_3.axes[0].set_title('Scale-Location')\n",
    "plot_lm_3.axes[0].set_xlabel('Fitted values')\n",
    "plot_lm_3.axes[0].set_ylabel('$\\sqrt{|Standardized Residuals|}$');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cook’s Distance, we only need to find leverage points that have a distance greater than 0.5 (above dotted red line). \n",
    "\n",
    "plot_lm_4 = plt.figure();\n",
    "plt.scatter(model_leverage, model_norm_residuals, alpha=0.5);\n",
    "sns.regplot(model_leverage, model_norm_residuals,\n",
    "          scatter=False,\n",
    "          ci=False,\n",
    "          lowess=True,\n",
    "          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8});\n",
    "plot_lm_4.axes[0].set_xlim(0, max(model_leverage)+0.01)\n",
    "plot_lm_4.axes[0].set_ylim(-3, 5)\n",
    "plot_lm_4.axes[0].set_title('Residuals vs Leverage')\n",
    "plot_lm_4.axes[0].set_xlabel('Leverage')\n",
    "plot_lm_4.axes[0].set_ylabel('Standardized Residuals');\n",
    "\n",
    "# shenanigans for cook's distance....not sure this is right \n",
    "def graph(formula, x_range, label=None):\n",
    "    x = x_range\n",
    "    y = formula(x)\n",
    "    plt.plot(x, y, label=label, lw=1, ls='--', color='red')\n",
    "\n",
    "# annotations\n",
    "\n",
    "\n",
    "p = len(model_fit.params) # number of model parameters\n",
    "graph(lambda x: np.sqrt((0.5 * p * (1 - x)) / x),\n",
    "    np.linspace(0.001, max(model_leverage), 50),\n",
    "    'Cook\\'s distance') # 0.5 line\n",
    "\n",
    "#graph(lambda x: np.sqrt((1 * p * (1 - x)) / x),\n",
    "    #np.linspace(0.001, max(model_leverage), 50)) # 1 line\n",
    "\n",
    "plot_lm_4.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy import dmatrices\n",
    "import statsmodels.api as sm\n",
    "\n",
    "formula = \"\"\"dep ~ ind1 + C(ind2_catagorical) )\"\"\"\n",
    "\n",
    "response, predictors = dmatrices(formula, data, return_type='dataframe')\n",
    "po_results = sm.GLM(response, predictors, family=sm.families.Poisson()).fit()\n",
    "print(po_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poisson regression code\n",
    "import statsmodels.api as sm\n",
    "\n",
    "exog, endog = sm.add_constant(x), y\n",
    "\n",
    "mod = sm.GLM(endog, exog, family=sm.families.Poisson(link=sm.families.links.log))\n",
    "\n",
    "res = mod.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coor Analysis\n",
    "\n",
    "create a clustering coef.\n",
    "net measure predicts feelings about carbon tax\n",
    "show optimal binning variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
